{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cosmos.initialization] Creating SSH connection to alogin1.bsc.es\n",
      "[cosmos.initialization] Connection established and remote path '/gpfs/projects/bsc14/executions' verified.\n"
     ]
    }
   ],
   "source": [
    "import cosmos\n",
    "from cosmos.execution_types import TRAINING_MODEL\n",
    "\n",
    "cosmos.initialization(host=\"alogin1.bsc.es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cosmos.run(\n",
    "    module_path=\"trans_cpt.preprocessing\",\n",
    "    function_name=\"get_dataset\",\n",
    "    queue=\"acc_debug\",\n",
    "    user=\"bsc14\",\n",
    "    kwargs={\n",
    "        \"repository\": \"huggingface\",\n",
    "        \"dataset_name\": \"DT4H/the_chilean_waiting_list_corpus\"\n",
    "    },\n",
    "    requirements=[\"python-dotenv\", \"datasets\", \"fsspec\"],\n",
    "    modules=[],\n",
    "    partition=\"debug\",\n",
    "    nodes=1,\n",
    "    cpus=20,\n",
    "    gpus=1,\n",
    "    venv_path=\"/gpfs/projects/bsc14/environments/trans_cpt\",\n",
    "    watch=True,\n",
    "    execute_with_slurm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cosmos.run] Preparing configuration to execute 'training_pipeline'\n",
      "[cosmos.run] Creating virtual environment\n",
      "[cosmos.run] All requirements already installed\n",
      "[cosmos.run] Environment ready in /gpfs/projects/bsc14/environments/trans_cpt. Requirements: ['datasets', 'transformers', 'torch', 'accelerate', 'tqdm', 'tensorboard']\n",
      "[cosmos.run] Output remote logs: /gpfs/projects/bsc14/executions/job_20250205_152231/job_20250205_152231.out\n",
      "[cosmos.run] Error remote logs: /gpfs/projects/bsc14/executions/job_20250205_152231/job_20250205_152231.err\n",
      "[cosmos.run] Submitted batch job 15232383\n",
      "[cosmos.run] Job job_20250205_152231 (ID: 15232383) sent.\n",
      "\n",
      "[job.out_file] === Dependencies installed ===                                                       \n",
      "[job.out_file] absl-py==2.1.0\n",
      "[job.out_file] accelerate==1.3.0\n",
      "[job.out_file] aiohappyeyeballs==2.4.4\n",
      "[job.out_file] aiohttp==3.11.11\n",
      "[job.out_file] aiosignal==1.3.2\n",
      "[job.out_file] attrs==24.3.0\n",
      "[job.out_file] certifi==2024.12.14\n",
      "[job.out_file] charset-normalizer==3.4.1\n",
      "[job.out_file] datasets==3.2.0\n",
      "[job.out_file] dill==0.3.8\n",
      "[job.out_file] filelock==3.17.0\n",
      "[job.out_file] frozenlist==1.5.0\n",
      "[job.out_file] fsspec==2024.9.0\n",
      "[job.out_file] grpcio==1.69.0\n",
      "[job.out_file] huggingface-hub==0.27.1\n",
      "[job.out_file] idna==3.10\n",
      "[job.out_file] Jinja2==3.1.5\n",
      "[job.out_file] Markdown==3.7\n",
      "[job.out_file] MarkupSafe==3.0.2\n",
      "[job.out_file] mpmath==1.3.0\n",
      "[job.out_file] multidict==6.1.0\n",
      "[job.out_file] multiprocess==0.70.16\n",
      "[job.out_file] networkx==3.4.2\n",
      "[job.out_file] numpy==2.2.2\n",
      "[job.out_file] nvidia-cublas-cu12==12.4.5.8\n",
      "[job.out_file] nvidia-cuda-cupti-cu12==12.4.127\n",
      "[job.out_file] nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "[job.out_file] nvidia-cuda-runtime-cu12==12.4.127\n",
      "[job.out_file] nvidia-cudnn-cu12==9.1.0.70\n",
      "[job.out_file] nvidia-cufft-cu12==11.2.1.3\n",
      "[job.out_file] nvidia-curand-cu12==10.3.5.147\n",
      "[job.out_file] nvidia-cusolver-cu12==11.6.1.9\n",
      "[job.out_file] nvidia-cusparse-cu12==12.3.1.170\n",
      "[job.out_file] nvidia-nccl-cu12==2.21.5\n",
      "[job.out_file] nvidia-nvjitlink-cu12==12.4.127\n",
      "[job.out_file] nvidia-nvtx-cu12==12.4.127\n",
      "[job.out_file] packaging==24.2\n",
      "[job.out_file] pandas==2.2.3\n",
      "[job.out_file] propcache==0.2.1\n",
      "[job.out_file] protobuf==5.29.3\n",
      "[job.out_file] psutil==6.1.1\n",
      "[job.out_file] pyarrow==19.0.0\n",
      "[job.out_file] python-dateutil==2.9.0.post0\n",
      "[job.out_file] python-dotenv==1.0.1\n",
      "[job.out_file] pytz==2024.2\n",
      "[job.out_file] PyYAML==6.0.2\n",
      "[job.out_file] regex==2024.11.6\n",
      "[job.out_file] requests==2.32.3\n",
      "[job.out_file] safetensors==0.5.2\n",
      "[job.out_file] six==1.17.0\n",
      "[job.out_file] sympy==1.13.1\n",
      "[job.out_file] tensorboard==2.18.0\n",
      "[job.out_file] tensorboard-data-server==0.7.2\n",
      "[job.out_file] tokenizers==0.21.0\n",
      "[job.out_file] torch==2.5.1\n",
      "[job.out_file] tqdm==4.67.1\n",
      "[job.out_file] transformers==4.48.1\n",
      "[job.out_file] triton==3.1.0\n",
      "[job.out_file] typing_extensions==4.12.2\n",
      "[job.out_file] tzdata==2025.1\n",
      "[job.out_file] urllib3==2.3.0\n",
      "[job.out_file] Werkzeug==3.1.3\n",
      "[job.out_file] xxhash==3.5.0\n",
      "[job.out_file] yarl==1.18.3\n",
      "[job.out_file] \n",
      "[job.out_file] =========================================\n",
      "[job.out_file] \n",
      "[job.err_file] load CUDA/12.6 (PATH, MANPATH, LD_LIBRARY_PATH, LIBRARY_PATH, C_INCLUDE_PATH,        \n",
      "[job.err_file] CPLUS_INCLUDE_PATH, CUDA_HOME, CUDA_VERSION, CUDA_INC, CUDA_INSTALL_PATH) \n",
      "[job.err_file] \n",
      "[job.out_file] Input variables: {'data_path': '/gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f'}\n",
      "[job.out_file] Input variables: {'data_path': '/gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f'}\n",
      "[job.out_file] Input variables: {'data_path': '/gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f'}\n",
      "[job.out_file] Input variables: {'data_path': '/gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f'}\n",
      "[job.out_file] Loading dataset from /gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f\n",
      "[job.out_file] Loading dataset from /gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f\n",
      "[job.out_file] Loading dataset from /gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f\n",
      "[job.out_file] Loading model from /gpfs/projects/bsc14/hf_models/roberta-base-biomedical-clinical-es\n",
      "[job.out_file] Loading model from /gpfs/projects/bsc14/hf_models/roberta-base-biomedical-clinical-es\n",
      "[job.out_file] Loading model from /gpfs/projects/bsc14/hf_models/roberta-base-biomedical-clinical-es\n",
      "[job.out_file] Loading dataset from /gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f\n",
      "[job.out_file] Loading model from /gpfs/projects/bsc14/hf_models/roberta-base-biomedical-clinical-es\n",
      "[job.out_file] Tokenizing the dataset\n",
      "[job.out_file] Tokenizing the dataset\n",
      "[job.out_file] Tokenizing the dataset\n",
      "[job.out_file] Tokenizing the dataset\n",
      "[job.out_file] Splitting the datasetSplitting the datasetSplitting the dataset\n",
      "[job.out_file] \n",
      "[job.out_file] \n",
      "[job.out_file] Train size: 1908Train size: 1908\n",
      "[job.out_file] Train size: 1908\n",
      "[job.out_file] Test size: 381\n",
      "[job.out_file] Test size: 381\n",
      "[job.out_file] Test size: 381\n",
      "[job.out_file] \n",
      "[job.out_file] Splitting the dataset\n",
      "[job.out_file] Train size: 1908\n",
      "[job.out_file] Test size: 381\n",
      "[job.out_file] input_ids: [1794, 11662, 290, 313, 8861, 2330, 288, 339, 9113, 51999, 297, 1674, 262, 284, 2302, 262, 318, 5629, 262, 6942]input_ids: [1794, 11662, 290, 313, 8861, 2330, 288, 339, 9113, 51999, 297, 1674, 262, 284, 2302, 262, 318, 5629, 262, 6942]input_ids: [1794, 11662, 290, 313, 8861, 2330, 288, 339, 9113, 51999, 297, 1674, 262, 284, 2302, 262, 318, 5629, 262, 6942]input_ids: [1794, 11662, 290, 313, 8861, 2330, 288, 339, 9113, 51999, 297, 1674, 262, 284, 2302, 262, 318, 5629, 262, 6942]\n",
      "[job.out_file] \n",
      "[job.out_file] \n",
      "[job.out_file] \n",
      "[job.out_file] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[job.out_file] \n",
      "[job.out_file] labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, 299, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[job.out_file] labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, 299, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[job.out_file] \n",
      "[job.out_file] labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, 299, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[job.out_file] labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, 299, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[job.out_file] \n",
      "[job.out_file] \n",
      "[job.out_file] Using 4 device(s).                                                                   \n",
      "[job.out_file] Using 4 device(s).\n",
      "[job.out_file] Using 4 device(s).\n",
      "[job.out_file] Using 4 device(s).\n",
      "[job.out_file] >>> TRAIN: Epoch 0 -- Loss: 1.260944 -- Average Training Perplexity: 3.528752\n",
      "[job.out_file] >>> EVAL: Epoch 0 -- Loss: 1.099814 -- Perplexity: 3.003606\n",
      "[job.out_file] >>> TRAIN: Epoch 1 -- Loss: 1.169609 -- Average Training Perplexity: 3.220734\n",
      "[job.out_file] >>> EVAL: Epoch 1 -- Loss: 1.081022 -- Perplexity: 2.947691\n",
      "[job.out_file] \n",
      " 16%|█▌        | 24/150 [00:06<00:28,  4.49it/s]                                                    \n",
      "[job.out_file] >>> TRAIN: Epoch 2 -- Loss: 1.143407 -- Average Training Perplexity: 3.137439        \n",
      "[job.out_file] >>> EVAL: Epoch 2 -- Loss: 1.070429 -- Perplexity: 2.916630\n",
      "[job.out_file] >>> TRAIN: Epoch 3 -- Loss: 1.115623 -- Average Training Perplexity: 3.051468\n",
      "[job.out_file] >>> EVAL: Epoch 3 -- Loss: 1.060094 -- Perplexity: 2.886642\n",
      "[job.out_file] >>> TRAIN: Epoch 4 -- Loss: 1.088997 -- Average Training Perplexity: 2.971292\n",
      "[job.out_file] >>> EVAL: Epoch 4 -- Loss: 1.059175 -- Perplexity: 2.883992\n",
      "[job.out_file] >>> TRAIN: Epoch 5 -- Loss: 1.102122 -- Average Training Perplexity: 3.010548\n",
      "[job.out_file] >>> EVAL: Epoch 5 -- Loss: 1.050718 -- Perplexity: 2.859704\n",
      "[job.out_file] \n",
      " 34%|███▍      | 51/150 [00:13<00:25,  3.84it/s]                                                    \n",
      "[job.out_file] >>> TRAIN: Epoch 6 -- Loss: 1.068468 -- Average Training Perplexity: 2.910916        \n",
      "[job.out_file] >>> EVAL: Epoch 6 -- Loss: 1.046385 -- Perplexity: 2.847338\n",
      "[job.out_file] >>> TRAIN: Epoch 7 -- Loss: 1.037790 -- Average Training Perplexity: 2.822971\n",
      "[job.out_file] >>> EVAL: Epoch 7 -- Loss: 1.046901 -- Perplexity: 2.848810\n",
      "[job.out_file] >>> TRAIN: Epoch 8 -- Loss: 1.046616 -- Average Training Perplexity: 2.847997\n",
      "[job.out_file] >>> EVAL: Epoch 8 -- Loss: 1.041835 -- Perplexity: 2.834414\n",
      "[job.out_file] \n",
      " 52%|█████▏    | 78/150 [00:20<00:17,  4.16it/s]                                                    \n",
      "[job.out_file] >>> TRAIN: Epoch 9 -- Loss: 1.026358 -- Average Training Perplexity: 2.790882        \n",
      "[job.out_file] >>> EVAL: Epoch 9 -- Loss: 1.041709 -- Perplexity: 2.834057\n",
      "[job.out_file] >>> TRAIN: Epoch 10 -- Loss: 0.997885 -- Average Training Perplexity: 2.712539\n",
      "[job.out_file] >>> EVAL: Epoch 10 -- Loss: 1.039279 -- Perplexity: 2.827178\n",
      "[job.out_file] >>> TRAIN: Epoch 11 -- Loss: 0.984985 -- Average Training Perplexity: 2.677771\n",
      "[job.out_file] >>> EVAL: Epoch 11 -- Loss: 1.039427 -- Perplexity: 2.827596\n",
      "[job.out_file] \n",
      " 69%|██████▉   | 104/150 [00:26<00:10,  4.45it/s]                                                   \n",
      "[job.out_file] >>> TRAIN: Epoch 12 -- Loss: 1.019756 -- Average Training Perplexity: 2.772518       \n",
      "[job.out_file] >>> EVAL: Epoch 12 -- Loss: 1.038902 -- Perplexity: 2.826113\n",
      "[job.out_file] >>> TRAIN: Epoch 13 -- Loss: 1.000896 -- Average Training Perplexity: 2.720720\n",
      "[job.out_file] >>> EVAL: Epoch 13 -- Loss: 1.040770 -- Perplexity: 2.831397\n",
      "[job.out_file] >>> TRAIN: Epoch 14 -- Loss: 0.982959 -- Average Training Perplexity: 2.672351\n",
      "[job.out_file] >>> EVAL: Epoch 14 -- Loss: 1.038582 -- Perplexity: 2.825207\n",
      "[job.out_file] >>> TRAIN: Epoch 15 -- Loss: 0.968708 -- Average Training Perplexity: 2.634538\n",
      "[job.out_file] >>> EVAL: Epoch 15 -- Loss: 1.037836 -- Perplexity: 2.823101\n",
      "[job.out_file] \n",
      " 87%|████████▋ | 130/150 [00:33<00:05,  3.69it/s]                                                   \n",
      "[job.out_file] >>> TRAIN: Epoch 16 -- Loss: 0.947501 -- Average Training Perplexity: 2.579255       \n",
      "[job.out_file] >>> EVAL: Epoch 16 -- Loss: 1.035966 -- Perplexity: 2.817827\n",
      "[job.out_file] >>> TRAIN: Epoch 17 -- Loss: 0.961699 -- Average Training Perplexity: 2.616136\n",
      "[job.out_file] >>> EVAL: Epoch 17 -- Loss: 1.039449 -- Perplexity: 2.827659\n",
      "[job.out_file] >>> TRAIN: Epoch 18 -- Loss: 0.916494 -- Average Training Perplexity: 2.500509\n",
      "[job.out_file] >>> EVAL: Epoch 18 -- Loss: 1.039093 -- Perplexity: 2.826653\n",
      "[job.out_file] \n",
      "157it [00:40,  4.10it/s]                                                                            \n",
      "[job.out_file] >>> TRAIN: Epoch 19 -- Loss: 0.942682 -- Average Training Perplexity: 2.566857       \n",
      "[job.out_file] >>> EVAL: Epoch 19 -- Loss: 1.037417 -- Perplexity: 2.821918\n",
      "[job.out_file] Saving model\n",
      "[job.out_file] Saving model\n",
      "[job.out_file] Saving model\n",
      "[job.out_file] Saving model\n",
      "[job.out_file] Model saved in /gpfs/projects/bsc14/storage/models/transcpt/CardioBERTa_2025-02-05_15-23-36\n",
      "[job.out_file] Model saved in /gpfs/projects/bsc14/storage/models/transcpt/CardioBERTa_2025-02-05_15-23-36\n",
      "[job.out_file] Model saved in /gpfs/projects/bsc14/storage/models/transcpt/CardioBERTa_2025-02-05_15-23-36\n",
      "[job.out_file] Model saved in /gpfs/projects/bsc14/storage/models/transcpt/CardioBERTa_2025-02-05_15-23-36\n",
      "[job.out_file] \n",
      "160it [00:41,  3.88it/s]                                                                            \n",
      "160it [00:41,  3.88it/s]\n",
      "160it [00:41,  3.88it/s]\n",
      "160it [00:41,  3.88it/s]\n",
      "[job.err_file] \n",
      "[job 15232383][2025-02-05 15:24:37.390568] Status: DONE - Time execution: 0:02:00.623751090055\n",
      "[cosmos.run] Job finished.\n",
      "[cosmos.run] Check logs in /gpfs/projects/bsc14/executions/job_20250205_152231\n",
      "[cosmos.run] Copying training logs from remote '/gpfs/projects/bsc14/executions/job_20250205_152231/./logs' to '/Users/pabloarancibiabarahona/Development/master_thesis/TransCPT/logs/job_20250205_152231'\n",
      "[_copy_folder_from_remote] Error al crear tar remoto: load bsc/1.0 (PATH, MANPATH)\n",
      "tar: logs: Cannot stat: No such file or directory\n",
      "tar: Exiting with failure status due to previous errors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job = cosmos.run(\n",
    "    module_path=\"trans_cpt.training\",\n",
    "    function_name=\"training_pipeline\",\n",
    "    queue=\"acc_debug\",\n",
    "    user=\"bsc14\",\n",
    "    args=[{\n",
    "        \"data_path\": \"/gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f\",\n",
    "    }],\n",
    "    requirements=[\n",
    "        \"datasets\",\n",
    "        \"transformers\",\n",
    "        \"torch\",\n",
    "        \"accelerate\",\n",
    "        \"tqdm\",\n",
    "        \"tensorboard\"\n",
    "    ],\n",
    "    modules=[\n",
    "        \"cuda/12.6\"\n",
    "    ],\n",
    "    partition=\"debug\",\n",
    "    nodes=1,\n",
    "    cpus=80,\n",
    "    gpus=4,\n",
    "    venv_path=\"/gpfs/projects/bsc14/environments/trans_cpt\",\n",
    "    custom_command=\"accelerate launch --config_file ./trans_cpt/accelerate_config.yaml\",\n",
    "    execution_type=TRAINING_MODEL,\n",
    "    training_logs_path=\"training_logs\",\n",
    "    watch=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos.check_status(job)\n",
    "# cosmos.print_logs(job)\n",
    "# cosmos.cancel_job(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cosmos.run(\n",
    "    module_path=\"trans_cpt.training\",\n",
    "    function_name=\"inference_pipeline\",\n",
    "    queue=\"acc_debug\",\n",
    "    user=\"bsc14\",\n",
    "    args=[{\n",
    "        \"model_path\": \"/gpfs/projects/bsc14/storage/models/transcpt/CardioBERTa_2025-01-17_15-27-01\",\n",
    "        \"text\": (\n",
    "            \"Con el diagnóstico de endocarditis infecciosa sobre válvula protésica por Bacteroides fragilis,\"\n",
    "            \"se comenzó tratamiento con metronidazol 500 mg/8 horas y amoxicilina-clavulánico 1000 mg/200mg/8 \"\n",
    "            \"horas intravenoso. La paciente permaneció <mask> durante todo el ingreso, senegativizaron los hemocultivos \"\n",
    "            \"de forma precoz y evolucionó de forma favorables de su ligera descompensación cardiaca con tratamiento\"\n",
    "            \"diurético. Tras 6 semanas de tratamiento antibiótico intravenoso dirigido, estando estable hemodinámicamente \"\n",
    "            \"y en buena clase funcional se dio de alta hospitalaria.\"\n",
    "        ),\n",
    "    }],\n",
    "    requirements=[\n",
    "        \"datasets\",\n",
    "        \"transformers\",\n",
    "        \"torch\",\n",
    "        \"accelerate\",\n",
    "        \"tqdm\",\n",
    "        \"tensorboard\"\n",
    "    ],\n",
    "    modules=[\n",
    "        \"cuda/12.6\"\n",
    "    ],\n",
    "    partition=\"debug\",\n",
    "    nodes=1,\n",
    "    cpus=80,\n",
    "    gpus=4,\n",
    "    venv_path=\"/gpfs/projects/bsc14/environments/trans_cpt\",\n",
    "    custom_command=\"accelerate launch --config_file ./trans_cpt/accelerate_config.yaml\",\n",
    "    watch=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
