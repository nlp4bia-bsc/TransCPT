{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cosmos.initialization] Creating SSH connection to alogin4.bsc.es\n",
      "[cosmos.initialization] Connection established and remote path '/gpfs/projects/bsc14/executions' verified.\n"
     ]
    }
   ],
   "source": [
    "import cosmos\n",
    "\n",
    "cosmos.initialization(host=\"alogin4.bsc.es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cosmos.run] Preparing configuration to execute 'get_dataset'\n",
      "[cosmos.run] Creating virtual environment\n",
      "[cosmos.run] All requirements already installed\n",
      "[cosmos.run] Environment ready in /gpfs/projects/bsc14/environments/trans_cpt. Requirements: ['python-dotenv']\n",
      "[cosmos.run] Output remote logs: /gpfs/projects/bsc14/executions/job_20250121_161429/job_20250121_161429.out\n",
      "[cosmos.run] Error remote logs: /gpfs/projects/bsc14/executions/job_20250121_161429/job_20250121_161429.err\n",
      "[cosmos.run] Submitted batch job 14239469\n",
      "[cosmos.run] Job job_20250121_161429 (ID: 14239469) sent.\n",
      "\n",
      "[job.out_file] === Dependencies installed ===                                                       \n",
      "[job.out_file] absl-py==2.1.0\n",
      "[job.out_file] accelerate==1.2.1\n",
      "[job.out_file] aiohappyeyeballs==2.4.4\n",
      "[job.out_file] aiohttp==3.11.11\n",
      "[job.out_file] aiosignal==1.3.2\n",
      "[job.out_file] attrs==24.3.0\n",
      "[job.out_file] certifi==2024.12.14\n",
      "[job.out_file] charset-normalizer==3.4.1\n",
      "[job.out_file] datasets==3.2.0\n",
      "[job.out_file] dill==0.3.8\n",
      "[job.out_file] filelock==3.16.1\n",
      "[job.out_file] frozenlist==1.5.0\n",
      "[job.out_file] fsspec==2024.9.0\n",
      "[job.out_file] grpcio==1.69.0\n",
      "[job.out_file] huggingface-hub==0.27.1\n",
      "[job.out_file] idna==3.10\n",
      "[job.out_file] Jinja2==3.1.5\n",
      "[job.out_file] Markdown==3.7\n",
      "[job.out_file] MarkupSafe==3.0.2\n",
      "[job.out_file] mpmath==1.3.0\n",
      "[job.out_file] multidict==6.1.0\n",
      "[job.out_file] multiprocess==0.70.16\n",
      "[job.out_file] networkx==3.4.2\n",
      "[job.out_file] numpy==2.2.1\n",
      "[job.out_file] nvidia-cublas-cu12==12.4.5.8\n",
      "[job.out_file] nvidia-cuda-cupti-cu12==12.4.127\n",
      "[job.out_file] nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "[job.out_file] nvidia-cuda-runtime-cu12==12.4.127\n",
      "[job.out_file] nvidia-cudnn-cu12==9.1.0.70\n",
      "[job.out_file] nvidia-cufft-cu12==11.2.1.3\n",
      "[job.out_file] nvidia-curand-cu12==10.3.5.147\n",
      "[job.out_file] nvidia-cusolver-cu12==11.6.1.9\n",
      "[job.out_file] nvidia-cusparse-cu12==12.3.1.170\n",
      "[job.out_file] nvidia-nccl-cu12==2.21.5\n",
      "[job.out_file] nvidia-nvjitlink-cu12==12.4.127\n",
      "[job.out_file] nvidia-nvtx-cu12==12.4.127\n",
      "[job.out_file] packaging==24.2\n",
      "[job.out_file] pandas==2.2.3\n",
      "[job.out_file] propcache==0.2.1\n",
      "[job.out_file] protobuf==5.29.3\n",
      "[job.out_file] psutil==6.1.1\n",
      "[job.out_file] pyarrow==19.0.0\n",
      "[job.out_file] python-dateutil==2.9.0.post0\n",
      "[job.out_file] python-dotenv==1.0.1\n",
      "[job.out_file] pytz==2024.2\n",
      "[job.out_file] PyYAML==6.0.2\n",
      "[job.out_file] regex==2024.11.6\n",
      "[job.out_file] requests==2.32.3\n",
      "[job.out_file] safetensors==0.5.2\n",
      "[job.out_file] six==1.17.0\n",
      "[job.out_file] sympy==1.13.1\n",
      "[job.out_file] tensorboard==2.18.0\n",
      "[job.out_file] tensorboard-data-server==0.7.2\n",
      "[job.out_file] tokenizers==0.21.0\n",
      "[job.out_file] torch==2.5.1\n",
      "[job.out_file] tqdm==4.67.1\n",
      "[job.out_file] transformers==4.48.0\n",
      "[job.out_file] triton==3.1.0\n",
      "[job.out_file] typing_extensions==4.12.2\n",
      "[job.out_file] tzdata==2024.2\n",
      "[job.out_file] urllib3==2.3.0\n",
      "[job.out_file] Werkzeug==3.1.3\n",
      "[job.out_file] xxhash==3.5.0\n",
      "[job.out_file] yarl==1.18.3\n",
      "[job.out_file] \n",
      "[job.out_file] =========================================\n",
      "[job.out_file] huggingface\n",
      "[job.out_file] DT4H/cardiology_dataset_es\n",
      "[job.out_file] /gpfs/projects/bsc14/executions/storage\n",
      "[job.out_file] \n",
      "[job.err_file] Traceback (most recent call last):                                                   \n",
      "[job.err_file]   File \"/gpfs/projects/bsc14/executions/job_20250121_161429/entry_script.py\", line 58, in <module>\n",
      "[job.err_file]     func(*args, **kwargs)\n",
      "[job.err_file]   File \"/gpfs/projects/bsc14/executions/job_20250121_161429/trans_cpt/preprocessing.py\", line 21, in get_dataset\n",
      "[job.err_file]     controller.download_dataset()\n",
      "[job.err_file]   File \"/gpfs/projects/bsc14/executions/job_20250121_161429/trans_cpt/adapters/huggingface.py\", line 20, in download_dataset\n",
      "[job.err_file]     dataset = load_dataset(self.dataset_name, use_auth_token=os.getenv(\"HF_TOKEN\"))\n",
      "[job.err_file]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[job.err_file]   File \"/gpfs/projects/bsc14/environments/trans_cpt/lib/python3.11/site-packages/datasets/load.py\", line 2129, in load_dataset\n",
      "[job.err_file]     builder_instance = load_dataset_builder(\n",
      "[job.err_file]                        ^^^^^^^^^^^^^^^^^^^^^\n",
      "[job.err_file]   File \"/gpfs/projects/bsc14/environments/trans_cpt/lib/python3.11/site-packages/datasets/load.py\", line 1849, in load_dataset_builder\n",
      "[job.err_file]     dataset_module = dataset_module_factory(\n",
      "[job.err_file]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[job.err_file]   File \"/gpfs/projects/bsc14/environments/trans_cpt/lib/python3.11/site-packages/datasets/load.py\", line 1731, in dataset_module_factory\n",
      "[job.err_file]     raise e1 from None\n",
      "[job.err_file]   File \"/gpfs/projects/bsc14/environments/trans_cpt/lib/python3.11/site-packages/datasets/load.py\", line 1618, in dataset_module_factory\n",
      "[job.err_file]     raise ConnectionError(f\"Couldn't reach '{path}' on the Hub ({e.__class__.__name__})\") from e\n",
      "[job.err_file] ConnectionError: Couldn't reach 'DT4H/cardiology_dataset_es' on the Hub (LocalEntryNotFoundError)\n",
      "[job.err_file] \n",
      "[job 14239469][2025-01-21 16:14:55.048146] Status: DONE - Time execution: 0:00:18.181030\n",
      "[cosmos.run] Job finished.\n",
      "[cosmos.run] Check logs in /gpfs/projects/bsc14/executions/job_20250121_161429\n"
     ]
    }
   ],
   "source": [
    "result = cosmos.run(\n",
    "    module_path=\"trans_cpt.preprocessing\",\n",
    "    function_name=\"get_dataset\",\n",
    "    queue=\"acc_debug\",\n",
    "    user=\"bsc14\",\n",
    "    kwargs={\n",
    "        \"repository\": \"huggingface\",\n",
    "        \"dataset_name\": \"DT4H/cardiology_dataset_es\"\n",
    "    },\n",
    "    requirements=[\"python-dotenv\"],\n",
    "    modules=[],\n",
    "    partition=\"debug\",\n",
    "    nodes=1,\n",
    "    cpus=20,\n",
    "    gpus=1,\n",
    "    venv_path=\"/gpfs/projects/bsc14/environments/trans_cpt\",\n",
    "    watch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cosmos.run] Preparing configuration to execute 'training_pipeline'\n",
      "[cosmos.run] Creating virtual environment\n",
      "[cosmos.run] All requirements already installed\n",
      "[cosmos.run] Environment ready in /gpfs/projects/bsc14/environments/trans_cpt. Requirements: ['datasets', 'transformers', 'torch', 'accelerate', 'tqdm', 'tensorboard']\n",
      "[cosmos.run] Output remote logs: /gpfs/projects/bsc14/executions/job_20250121_163614/job_20250121_163614.out\n",
      "[cosmos.run] Error remote logs: /gpfs/projects/bsc14/executions/job_20250121_163614/job_20250121_163614.err\n",
      "[cosmos.run] Submitted batch job 14241529\n",
      "[cosmos.run] Job job_20250121_163614 (ID: 14241529) sent.\n",
      "\n",
      "[job.out_file] === Dependencies installed ===                                                       \n",
      "[job.out_file] absl-py==2.1.0\n",
      "[job.out_file] accelerate==1.2.1\n",
      "[job.out_file] aiohappyeyeballs==2.4.4\n",
      "[job.out_file] aiohttp==3.11.11\n",
      "[job.out_file] aiosignal==1.3.2\n",
      "[job.out_file] attrs==24.3.0\n",
      "[job.out_file] certifi==2024.12.14\n",
      "[job.out_file] charset-normalizer==3.4.1\n",
      "[job.out_file] datasets==3.2.0\n",
      "[job.out_file] dill==0.3.8\n",
      "[job.out_file] filelock==3.16.1\n",
      "[job.out_file] frozenlist==1.5.0\n",
      "[job.out_file] fsspec==2024.9.0\n",
      "[job.out_file] grpcio==1.69.0\n",
      "[job.out_file] huggingface-hub==0.27.1\n",
      "[job.out_file] idna==3.10\n",
      "[job.out_file] Jinja2==3.1.5\n",
      "[job.out_file] Markdown==3.7\n",
      "[job.out_file] MarkupSafe==3.0.2\n",
      "[job.out_file] mpmath==1.3.0\n",
      "[job.out_file] multidict==6.1.0\n",
      "[job.out_file] multiprocess==0.70.16\n",
      "[job.out_file] networkx==3.4.2\n",
      "[job.out_file] numpy==2.2.1\n",
      "[job.out_file] nvidia-cublas-cu12==12.4.5.8\n",
      "[job.out_file] nvidia-cuda-cupti-cu12==12.4.127\n",
      "[job.out_file] nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "[job.out_file] nvidia-cuda-runtime-cu12==12.4.127\n",
      "[job.out_file] nvidia-cudnn-cu12==9.1.0.70\n",
      "[job.out_file] nvidia-cufft-cu12==11.2.1.3\n",
      "[job.out_file] nvidia-curand-cu12==10.3.5.147\n",
      "[job.out_file] nvidia-cusolver-cu12==11.6.1.9\n",
      "[job.out_file] nvidia-cusparse-cu12==12.3.1.170\n",
      "[job.out_file] nvidia-nccl-cu12==2.21.5\n",
      "[job.out_file] nvidia-nvjitlink-cu12==12.4.127\n",
      "[job.out_file] nvidia-nvtx-cu12==12.4.127\n",
      "[job.out_file] packaging==24.2\n",
      "[job.out_file] pandas==2.2.3\n",
      "[job.out_file] propcache==0.2.1\n",
      "[job.out_file] protobuf==5.29.3\n",
      "[job.out_file] psutil==6.1.1\n",
      "[job.out_file] pyarrow==19.0.0\n",
      "[job.out_file] python-dateutil==2.9.0.post0\n",
      "[job.out_file] python-dotenv==1.0.1\n",
      "[job.out_file] pytz==2024.2\n",
      "[job.out_file] PyYAML==6.0.2\n",
      "[job.out_file] regex==2024.11.6\n",
      "[job.out_file] requests==2.32.3\n",
      "[job.out_file] safetensors==0.5.2\n",
      "[job.out_file] six==1.17.0\n",
      "[job.out_file] sympy==1.13.1\n",
      "[job.out_file] tensorboard==2.18.0\n",
      "[job.out_file] tensorboard-data-server==0.7.2\n",
      "[job.out_file] tokenizers==0.21.0\n",
      "[job.out_file] torch==2.5.1\n",
      "[job.out_file] tqdm==4.67.1\n",
      "[job.out_file] transformers==4.48.0\n",
      "[job.out_file] triton==3.1.0\n",
      "[job.out_file] typing_extensions==4.12.2\n",
      "[job.out_file] tzdata==2024.2\n",
      "[job.out_file] urllib3==2.3.0\n",
      "[job.out_file] Werkzeug==3.1.3\n",
      "[job.out_file] xxhash==3.5.0\n",
      "[job.out_file] yarl==1.18.3\n",
      "[job.out_file] \n",
      "[job.out_file] =========================================\n",
      "[job.out_file] \n",
      "[job.err_file] load CUDA/12.6 (PATH, MANPATH, LD_LIBRARY_PATH, LIBRARY_PATH, C_INCLUDE_PATH,        \n",
      "[job.err_file] CPLUS_INCLUDE_PATH, CUDA_HOME, CUDA_VERSION, CUDA_INC, CUDA_INSTALL_PATH) \n",
      "[job.err_file] \n",
      "[job.out_file] Input variables: {'data_path': '/gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f'}\n",
      "[job.out_file] Input variables: {'data_path': '/gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f'}\n",
      "[job.out_file] Loading dataset from /gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f\n",
      "[job.out_file] Loading model from /gpfs/projects/bsc14/hf_models/roberta-base-biomedical-clinical-es\n",
      "[job.out_file] Loading dataset from /gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f\n",
      "[job.out_file] Loading model from /gpfs/projects/bsc14/hf_models/roberta-base-biomedical-clinical-es\n",
      "[job.out_file] Tokenizing the dataset\n",
      "[job.out_file] Tokenizing the dataset\n",
      "[job.out_file] \n",
      "Map:   0%|          | 0/978 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1251 > 512). Running this sequence through the model will result in indexing errors\n",
      "[job.err_file] Token indices sequence length is longer than the specified maximum sequence length for this model (1251 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 1236.15 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 1143.50 examples/s]\n",
      "Map:   0%|          | 0/978 [00:00<?, ? examples/s]\n",
      "[job.out_file] Splitting the dataset                                                                \n",
      "[job.out_file] Train size: 1908\n",
      "[job.out_file] Test size: 381\n",
      "[job.out_file] Splitting the dataset\n",
      "[job.out_file] Train size: 1908\n",
      "[job.out_file] Test size: 381\n",
      "[job.out_file] input_ids: [1794, 11662, 290, 313, 8861, 2330, 288, 339, 9113, 51999, 297, 1674, 262, 284, 2302, 262, 318, 5629, 262, 6942]\n",
      "[job.out_file] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[job.out_file] labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, 299, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[job.out_file] input_ids: [1794, 11662, 290, 313, 8861, 2330, 288, 339, 9113, 51999, 297, 1674, 262, 284, 2302, 262, 318, 5629, 262, 6942]\n",
      "[job.out_file] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[job.out_file] labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, 299, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[job.out_file] Using 2 device(s).\n",
      "[job.out_file] Using 2 device(s).\n",
      "[job.out_file] \n",
      "Map: 100%|██████████| 978/978 [00:03<00:00, 271.87 examples/s]                                      \n",
      "Map: 100%|██████████| 978/978 [00:03<00:00, 257.16 examples/s]\n",
      "Map: 100%|██████████| 381/381 [00:00<00:00, 1761.14 examples/s]\n",
      "Map: 100%|██████████| 381/381 [00:00<00:00, 1597.17 examples/s]\n",
      "  2%|▏         | 5/300 [00:02<01:32,  3.18it/s]\n",
      "[job.out_file] >>> TRAIN: Epoch 0 -- Loss: 1.245600 -- Average Training Perplexity: 3.475019        \n",
      "[job.out_file] >>> EVAL: Epoch 0 -- Loss: 1.122242 -- Perplexity: 3.071733\n",
      "[job.out_file] \n",
      " 10%|█         | 30/300 [00:08<00:58,  4.59it/s]                                                    \n",
      "[job.out_file] >>> TRAIN: Epoch 1 -- Loss: 1.160845 -- Average Training Perplexity: 3.192630        \n",
      "[job.out_file] >>> EVAL: Epoch 1 -- Loss: 1.104477 -- Perplexity: 3.017645\n",
      "[job.out_file] >>> TRAIN: Epoch 2 -- Loss: 1.108761 -- Average Training Perplexity: 3.030602\n",
      "[job.out_file] >>> EVAL: Epoch 2 -- Loss: 1.089887 -- Perplexity: 2.973938\n",
      "[job.out_file] \n",
      " 18%|█▊        | 55/300 [00:14<00:56,  4.30it/s]                                                    \n",
      "[job.out_file] >>> TRAIN: Epoch 3 -- Loss: 1.095830 -- Average Training Perplexity: 2.991665        \n",
      "[job.out_file] >>> EVAL: Epoch 3 -- Loss: 1.087061 -- Perplexity: 2.965545\n",
      "[job.out_file] >>> TRAIN: Epoch 4 -- Loss: 1.077573 -- Average Training Perplexity: 2.937541\n",
      "[job.out_file] >>> EVAL: Epoch 4 -- Loss: 1.085050 -- Perplexity: 2.959587\n",
      "[job.out_file] \n",
      " 27%|██▋       | 80/300 [00:20<00:54,  4.03it/s]                                                    \n",
      "[job.out_file] >>> TRAIN: Epoch 5 -- Loss: 1.068024 -- Average Training Perplexity: 2.909623        \n",
      "[job.out_file] >>> EVAL: Epoch 5 -- Loss: 1.084379 -- Perplexity: 2.957604\n",
      "[job.out_file] \n",
      " 35%|███▌      | 105/300 [00:26<00:42,  4.58it/s]                                                   \n",
      "[job.out_file] >>> TRAIN: Epoch 6 -- Loss: 1.051873 -- Average Training Perplexity: 2.863009        \n",
      "[job.out_file] >>> EVAL: Epoch 6 -- Loss: 1.085937 -- Perplexity: 2.962214\n",
      "[job.out_file] >>> TRAIN: Epoch 7 -- Loss: 1.032144 -- Average Training Perplexity: 2.807077\n",
      "[job.out_file] >>> EVAL: Epoch 7 -- Loss: 1.083048 -- Perplexity: 2.953670\n",
      "[job.out_file] \n",
      " 43%|████▎     | 130/300 [00:32<00:39,  4.31it/s]                                                   \n",
      "[job.out_file] >>> TRAIN: Epoch 8 -- Loss: 1.024024 -- Average Training Perplexity: 2.784376        \n",
      "[job.out_file] >>> EVAL: Epoch 8 -- Loss: 1.081041 -- Perplexity: 2.947746\n",
      "[job.out_file] >>> TRAIN: Epoch 9 -- Loss: 1.017066 -- Average Training Perplexity: 2.765069\n",
      "[job.out_file] >>> EVAL: Epoch 9 -- Loss: 1.082003 -- Perplexity: 2.950583\n",
      "[job.out_file] \n",
      " 51%|█████▏    | 154/300 [00:38<00:37,  3.90it/s]                                                   \n",
      "[job.out_file] >>> TRAIN: Epoch 10 -- Loss: 0.986634 -- Average Training Perplexity: 2.682191       \n",
      "[job.out_file] >>> EVAL: Epoch 10 -- Loss: 1.080204 -- Perplexity: 2.945281\n",
      "[job.out_file] >>> TRAIN: Epoch 11 -- Loss: 0.981283 -- Average Training Perplexity: 2.667877\n",
      "[job.out_file] \n",
      " 61%|██████    | 182/300 [00:45<00:33,  3.50it/s]                                                   \n",
      "[job.out_file] >>> EVAL: Epoch 11 -- Loss: 1.077390 -- Perplexity: 2.937004                         \n",
      "[job.out_file] >>> TRAIN: Epoch 12 -- Loss: 0.974001 -- Average Training Perplexity: 2.648520\n",
      "[job.out_file] >>> EVAL: Epoch 12 -- Loss: 1.077946 -- Perplexity: 2.938637\n",
      "[job.out_file] \n",
      " 69%|██████▉   | 207/300 [00:51<00:21,  4.34it/s]                                                   \n",
      "[job.out_file] >>> TRAIN: Epoch 13 -- Loss: 0.956650 -- Average Training Perplexity: 2.602961       \n",
      "[job.out_file] >>> EVAL: Epoch 13 -- Loss: 1.078564 -- Perplexity: 2.940453\n",
      "[job.out_file] >>> TRAIN: Epoch 14 -- Loss: 0.961288 -- Average Training Perplexity: 2.615063\n",
      "[job.out_file] >>> EVAL: Epoch 14 -- Loss: 1.087578 -- Perplexity: 2.967078\n",
      "[job.out_file] \n",
      " 77%|███████▋  | 232/300 [00:58<00:16,  4.20it/s]                                                   \n",
      "[job.out_file] >>> TRAIN: Epoch 15 -- Loss: 0.936649 -- Average Training Perplexity: 2.551418       \n",
      "[job.out_file] >>> EVAL: Epoch 15 -- Loss: 1.082897 -- Perplexity: 2.953224\n",
      "[job.out_file] >>> TRAIN: Epoch 16 -- Loss: 0.933370 -- Average Training Perplexity: 2.543065\n",
      "[job.out_file] >>> EVAL: Epoch 16 -- Loss: 1.080106 -- Perplexity: 2.944993\n",
      "[job.out_file] \n",
      " 86%|████████▌ | 258/300 [01:04<00:11,  3.71it/s]                                                   \n",
      "[job.out_file] >>> TRAIN: Epoch 17 -- Loss: 0.919945 -- Average Training Perplexity: 2.509152       \n",
      "[job.out_file] >>> EVAL: Epoch 17 -- Loss: 1.081208 -- Perplexity: 2.948240\n",
      "[job.out_file] >>> TRAIN: Epoch 18 -- Loss: 0.915683 -- Average Training Perplexity: 2.498481\n",
      "[job.out_file] >>> EVAL: Epoch 18 -- Loss: 1.084584 -- Perplexity: 2.958209\n",
      "[job.out_file] \n",
      " 96%|█████████▋| 289/300 [01:12<00:02,  3.90it/s]                                                   \n",
      "[job.out_file] >>> TRAIN: Epoch 19 -- Loss: 0.905946 -- Average Training Perplexity: 2.474273       \n",
      "[job.out_file] Saving model\n",
      "[job.out_file] >>> EVAL: Epoch 19 -- Loss: 1.085732 -- Perplexity: 2.961607\n",
      "[job.out_file] Saving model\n",
      "[job.out_file] Model saved in /gpfs/projects/bsc14/storage/models/transcpt/CardioBERTa_2025-01-21_16-37-50\n",
      "[job.out_file] Model saved in /gpfs/projects/bsc14/storage/models/transcpt/CardioBERTa_2025-01-21_16-37-50\n",
      "[job.out_file] \n",
      "100%|██████████| 300/300 [01:14<00:00,  4.01it/s]                                                   \n",
      "100%|██████████| 300/300 [01:14<00:00,  4.01it/s]\n",
      "[job.err_file] \n",
      "[job 14241529][2025-01-21 16:39:26.123421] Status: DONE - Time execution: 0:03:06.436565460\n",
      "[cosmos.run] Job finished.\n",
      "[cosmos.run] Check logs in /gpfs/projects/bsc14/executions/job_20250121_163614\n"
     ]
    }
   ],
   "source": [
    "result = cosmos.run(\n",
    "    module_path=\"trans_cpt.training\",\n",
    "    function_name=\"training_pipeline\",\n",
    "    queue=\"acc_debug\",\n",
    "    user=\"bsc14\",\n",
    "    args=[{\n",
    "        \"data_path\": \"/gpfs/projects/bsc14/abecerr1/datasets/DT4H___wikipedia_cardiology_es/default/0.0.0/b20f70bf02ea8c0f5e0181e333b7b9ab3c610c4f\",\n",
    "    }],\n",
    "    requirements=[\n",
    "        \"datasets\",\n",
    "        \"transformers\",\n",
    "        \"torch\",\n",
    "        \"accelerate\",\n",
    "        \"tqdm\",\n",
    "        \"tensorboard\"\n",
    "    ],\n",
    "    modules=[\n",
    "        \"cuda/12.6\"\n",
    "    ],\n",
    "    partition=\"debug\",\n",
    "    nodes=1,\n",
    "    cpus=80,\n",
    "    gpus=4,\n",
    "    venv_path=\"/gpfs/projects/bsc14/environments/trans_cpt\",\n",
    "    custom_command=\"accelerate launch --config_file ./trans_cpt/accelerate_config.yaml\",\n",
    "    watch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cosmos.run(\n",
    "    module_path=\"trans_cpt.training\",\n",
    "    function_name=\"inference_pipeline\",\n",
    "    queue=\"acc_debug\",\n",
    "    user=\"bsc14\",\n",
    "    args=[{\n",
    "        \"model_path\": \"/gpfs/projects/bsc14/storage/models/transcpt/CardioBERTa_2025-01-17_15-27-01\",\n",
    "        \"text\": (\n",
    "            \"Con el diagnóstico de endocarditis infecciosa sobre válvula protésica por Bacteroides fragilis,\"\n",
    "            \"se comenzó tratamiento con metronidazol 500 mg/8 horas y amoxicilina-clavulánico 1000 mg/200mg/8 \"\n",
    "            \"horas intravenoso. La paciente permaneció <mask> durante todo el ingreso, senegativizaron los hemocultivos \"\n",
    "            \"de forma precoz y evolucionó de forma favorables de su ligera descompensación cardiaca con tratamiento\"\n",
    "            \"diurético. Tras 6 semanas de tratamiento antibiótico intravenoso dirigido, estando estable hemodinámicamente \"\n",
    "            \"y en buena clase funcional se dio de alta hospitalaria.\"\n",
    "        ),\n",
    "    }],\n",
    "    requirements=[\n",
    "        \"datasets\",\n",
    "        \"transformers\",\n",
    "        \"torch\",\n",
    "        \"accelerate\",\n",
    "        \"tqdm\",\n",
    "        \"tensorboard\"\n",
    "    ],\n",
    "    modules=[\n",
    "        \"cuda/12.6\"\n",
    "    ],\n",
    "    partition=\"debug\",\n",
    "    nodes=1,\n",
    "    cpus=80,\n",
    "    gpus=4,\n",
    "    venv_path=\"/gpfs/projects/bsc14/environments/trans_cpt\",\n",
    "    custom_command=\"accelerate launch --config_file ./trans_cpt/accelerate_config.yaml\",\n",
    "    watch=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
